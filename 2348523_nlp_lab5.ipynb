{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMalFi65cRQC0VK8TN9dH04",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshIsac/nlp_lab/blob/main/2348523_nlp_lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zh51qMJ40GZ",
        "outputId": "2041f0d8-29d1-4497-ad08-122374823a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_antonyms(word):\n",
        "  antonyms=[]\n",
        "  for syn in wordnet.synsets(word):\n",
        "    for lm in syn.lemmas():\n",
        "        if lm.antonyms():\n",
        "          antonyms.append(lm.antonyms()[0].name())\n",
        "          return list(antonyms)\n",
        "words = ['back', 'sit', 'ugly', 'alive', 'won']\n",
        "for word in words:\n",
        "    print(word + ':', get_antonyms(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRgaCkNV5Fm3",
        "outputId": "48aea21f-21b9-4b2f-b32f-8467b4d5d34d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "back: ['front']\n",
            "sit: ['stand']\n",
            "ugly: ['beautiful']\n",
            "alive: ['dead']\n",
            "won: ['lose']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import FrenchStemmer"
      ],
      "metadata": {
        "id": "MzbW1K-J6TKE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_french=\"Ayant terminé ses devoirs, il est allé se coucher.La porte étant fermée, nous avons dû attendre dehors.\" #Having finished his homework, he went to bed.#2)The door was closed so we had to wait outside.\n",
        "nltk.download('punkt')\n",
        "tokens = nltk.word_tokenize(text_french)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJMAsrQ978Z6",
        "outputId": "85bd37a4-0be2-4738-9862-296225b609dc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfAGqdDy9wRT",
        "outputId": "67396bd8-e07b-40d0-c0b1-3cc3077d3892"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ayant', 'terminé', 'ses', 'devoirs', ',', 'il', 'est', 'allé', 'se', 'coucher.La', 'porte', 'étant', 'fermée', ',', 'nous', 'avons', 'dû', 'attendre', 'dehors', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "french_st = FrenchStemmer()\n",
        "stem_word=[french_st.stem(words)for words in tokens]"
      ],
      "metadata": {
        "id": "URJWv4Nw9yUX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"stemmed words of non english words are:\",stem_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC5wH6Qz-JJa",
        "outputId": "3bd50f4b-0041-4d67-eec8-f7e3941c6ee6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stemmed words of non english words are: ['ayant', 'termin', 'se', 'devoir', ',', 'il', 'est', 'allé', 'se', 'coucher.l', 'port', 'étant', 'ferm', ',', 'nous', 'avon', 'dû', 'attendr', 'dehor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3)program for lemmatizing words Using WordNet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "tokens=[\"studies\",\"kidding\",\"bagging\",\"looking\",\"doggies\"]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemma_st={lemmatizer.lemmatize(token.lower()) for token in tokens}\n",
        "print(lemma_st)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgPyE_is-cMl",
        "outputId": "e9592720-e55d-4dc9-b1a2-965bb09f9431"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'study', 'bagging', 'looking', 'doggy', 'kidding'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, RegexpStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(language='english')\n",
        "regexp_stemmer = RegexpStemmer('ing$|s$|e$', min=4)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens=[\"studies\",\"kidding\",\"boogies\",\"looking\",\"dogs\"]\n",
        "\n",
        "# Applying different stemming algorithms to the words\n",
        "stemming_algorithms = {\n",
        "    'Porter Stemmer': [porter_stemmer.stem(words) for words in tokens],\n",
        "    'Lancaster Stemmer': [lancaster_stemmer.stem(words) for words in tokens],\n",
        "    'Snowball Stemmer': [snowball_stemmer.stem(words) for words in tokens],\n",
        "    'Regexp Stemmer': [regexp_stemmer.stem(words) for words in tokens]\n",
        "}\n",
        "# Lemmatizing each word in the list\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
        "# Display the original, stemmed, and lemmatized words using different stemming algorithms\n",
        "print(\"Original Words:\", tokens)\n",
        "for algorithm, stemmed_words in stemming_algorithms.items():\n",
        "    print(f\"{algorithm}:\", stemmed_words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxgqb8nhDurq",
        "outputId": "9ddfab39-96d6-426f-bd20-ffa9cd0252f3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['studies', 'kidding', 'boogies', 'looking', 'dogs']\n",
            "Porter Stemmer: ['studi', 'kid', 'boogi', 'look', 'dog']\n",
            "Lancaster Stemmer: ['study', 'kid', 'boog', 'look', 'dog']\n",
            "Snowball Stemmer: ['studi', 'kid', 'boogi', 'look', 'dog']\n",
            "Regexp Stemmer: ['studie', 'kidd', 'boogie', 'look', 'dog']\n",
            "Lemmatized Words: ['study', 'kid', 'boogie', 'look', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIR9V3WZLwx7",
        "outputId": "2c7cb97e-052c-4c07-faab-53cb8a0a7e6f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5)write a program for PoS tagging\n",
        "Sentence1=\"I am Joshwin Isac ,pursuing MSc AI and ML at christ university\"\n",
        "Sentence2=\"My hobbies are listening to music,gaming and\"\n",
        "Sentence3=\"I play football and cricket during my leisure time and I am a car craze\"\n",
        "\n",
        "tokens1=nltk.word_tokenize(Sentence1)\n",
        "tokens2=nltk.word_tokenize(Sentence2)\n",
        "tokens3= nltk.word_tokenize(Sentence3)\n",
        "# Perform Part-of-Speech (PoS) tagging on each tokenized sentence\n",
        "pos_tagged_tokens1 = nltk.pos_tag(tokens1)\n",
        "pos_tagged_tokens2 = nltk.pos_tag(tokens2)\n",
        "pos_tagged_tokens3 = nltk.pos_tag(tokens3)\n",
        "\n",
        "# Print the PoS-tagged tokens for clarity\n",
        "print(\"PoS-tagged tokens for Sentence 1:\")\n",
        "print(pos_tagged_tokens1)\n",
        "\n",
        "print(\"\\nPoS-tagged tokens for Sentence 2:\")\n",
        "print(pos_tagged_tokens2)\n",
        "\n",
        "print(\"\\nPoS-tagged tokens for Sentence 3:\")\n",
        "print(pos_tagged_tokens3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qECtZKzVFOan",
        "outputId": "0edc9a6c-52e2-4cb1-f206-c73f448f6e21"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PoS-tagged tokens for Sentence 1:\n",
            "[('I', 'PRP'), ('am', 'VBP'), ('Joshwin', 'NNP'), ('Isac', 'NNP'), (',', ','), ('pursuing', 'VBG'), ('MSc', 'NNP'), ('AI', 'NNP'), ('and', 'CC'), ('ML', 'NNP'), ('at', 'IN'), ('christ', 'NN'), ('university', 'NN')]\n",
            "\n",
            "PoS-tagged tokens for Sentence 2:\n",
            "[('My', 'PRP$'), ('hobbies', 'NNS'), ('are', 'VBP'), ('listening', 'VBG'), ('to', 'TO'), ('music', 'NN'), (',', ','), ('gaming', 'NN'), ('and', 'CC')]\n",
            "\n",
            "PoS-tagged tokens for Sentence 3:\n",
            "[('I', 'PRP'), ('play', 'VBP'), ('football', 'NN'), ('and', 'CC'), ('cricket', 'NN'), ('during', 'IN'), ('my', 'PRP$'), ('leisure', 'NN'), ('time', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('car', 'NN'), ('craze', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the English language model for spaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "u5fDpldbNmLu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6)a program to identify the Named Entity Recognition and also execute any of the tool\n",
        "# Define your sentences\n",
        "sentences = [\n",
        "    \"I am Joshwin Isac, pursuing MSc AI and ML at Christ University\",\n",
        "    \"My hobbies are listening to music, gaming, and coding.\",\n",
        "    \"I play football and cricket during my leisure time, and I am a car enthusiast.\"\n",
        "]\n",
        "\n",
        "# Process each sentence with spaCy and identify named entities\n",
        "for sentence in sentences:\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Extract named entities and their labels\n",
        "    named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "\n",
        "    # Print the sentence and its identified named entities\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    if named_entities:\n",
        "        print(\"Named Entities:\")\n",
        "        for entity, label in named_entities:\n",
        "            print(f\"- {entity} ({label})\")\n",
        "    else:\n",
        "        print(\"No named entities found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tnd2wIALKPDR",
        "outputId": "91fa56b7-cda8-41a8-ef27-ad40327d44bf"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: I am Joshwin Isac, pursuing MSc AI and ML at Christ University\n",
            "Named Entities:\n",
            "- Joshwin Isac (PERSON)\n",
            "- AI (ORG)\n",
            "- ML (ORG)\n",
            "- Christ University (ORG)\n",
            "\n",
            "Sentence: My hobbies are listening to music, gaming, and coding.\n",
            "No named entities found.\n",
            "\n",
            "Sentence: I play football and cricket during my leisure time, and I am a car enthusiast.\n",
            "No named entities found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6kzpNEqNCsj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}