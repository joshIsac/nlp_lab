{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2i7fnLoOumD9xRmReq1E0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshIsac/nlp_lab/blob/main/Similarity_analysis_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "similarity analysis: let us consider taking any of the two docucment having text inputed checks the similarity based on the occurences of the code or the frquency of words\n",
        "It measures the how much there is meaning of the two pieces of text ,which gives us the measure of two text in the document that are related semanticaly.\n",
        "Some of the similarity measuring techniques are cosine ,jaccard etc\n",
        "Some of the examples are plagiarism detection ,sentiment analysis,classification of documents"
      ],
      "metadata": {
        "id": "2YDcsLuXoz0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoGBfub_ovq0",
        "outputId": "7a2ee4d9-4fd8-4688-c2d0-a5f4a70701b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk. corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWxMAHits-wH",
        "outputId": "34b21d3e-50d3-431b-e305-52e3172d3bfe"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<WordListCorpusReader in '/root/nltk_data/corpora/stopwords'>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_doc1=\"dogs are cute and loyal\"\n",
        "text_doc2=\"dogs are cute and cheerful\"\n",
        "text_doc3=\"dogs are playful and loyal\"\n",
        "#tokenize the sentence\n",
        "token1=word_tokenize(text_doc1)\n",
        "print(\"tokenizedtext_doc1:\",token1)\n",
        "token2=word_tokenize(text_doc2)\n",
        "print(\"tokenizedtext_doc2:\",token2)\n",
        "token3=word_tokenize(text_doc3)\n",
        "print(\"tokenizedtext_doc3:\",token3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQIzcDYZtF0w",
        "outputId": "c01914ac-7cf2-4633-ac6f-0ae40048df89"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizedtext_doc1: ['dogs', 'are', 'cute', 'and', 'loyal']\n",
            "tokenizedtext_doc2: ['dogs', 'are', 'cute', 'and', 'cheerful']\n",
            "tokenizedtext_doc3: ['dogs', 'are', 'playful', 'and', 'loyal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=(set(token1))\n",
        "b=(set(token2))\n",
        "print (a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPKLw--AlH6I",
        "outputId": "0555ba5e-1344-4291-9023-80f7afdace32"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loyal', 'and', 'cute', 'are', 'dogs'}\n",
            "{'and', 'cheerful', 'cute', 'are', 'dogs'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemma_token1 = [lemmatizer.lemmatize(token) for token in token1]\n",
        "print(\"lemmatized token:\",token1)\n",
        "lemma_token2 = [lemmatizer.lemmatize(token) for token in token2]\n",
        "print(\"lemmatized token:\",token2)\n",
        "lemma_token3= [lemmatizer.lemmatize(token) for token in token3]\n",
        "print(\"lemmatized token:\",token3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMRGfyiBtMkJ",
        "outputId": "04290cb6-0dc7-4005-9b82-54d0d2ac8338"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemmatized token: ['dogs', 'are', 'cute', 'and', 'loyal']\n",
            "lemmatized token: ['dogs', 'are', 'cute', 'and', 'cheerful']\n",
            "lemmatized token: ['dogs', 'are', 'playful', 'and', 'loyal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "joined_tkn = itertools.chain(token1,token2,token3)\n",
        "joined_tkn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBfIZTCiyxrV",
        "outputId": "11d36964-f0f3-4035-e28a-e2eb3e99f710"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<itertools.chain at 0x7c5216b64b80>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#frequency distribution of common words\n",
        "from nltk.probability import FreqDist\n",
        "fdist1=FreqDist(joined_tkn)\n",
        "fdist1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7bCu7NhvFFk",
        "outputId": "bfefe872-0531-4667-d70c-1a9856fa1ba7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'dogs': 3, 'are': 3, 'and': 3, 'cute': 2, 'loyal': 2, 'cheerful': 1, 'playful': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist=FreqDist((token1+token2+token3))\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0vvPWAD0K3D",
        "outputId": "af4366a0-3cb9-48cd-f167-735f63aa07a1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'dogs': 3, 'are': 3, 'and': 3, 'cute': 2, 'loyal': 2, 'cheerful': 1, 'playful': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check for stopwords\n",
        "stopwords = list(stopwords.words('english'))\n",
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvt90CG80TMJ",
        "outputId": "85886a07-59cc-48ee-9c2f-6c1be0817e49"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering the stopwords\n",
        "tkn1 = [token for token in token1 if token  in stopwords]\n",
        "tkn2 = [token for token in token2 if token  in stopwords]\n",
        "tkn3 = [token for token in token3 if token  in stopwords]\n",
        "\n",
        "from collections import Counter\n",
        "# Count the frequency of each stopword\n",
        "stopword_count1 = Counter(tkn1)\n",
        "stopword_count1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA1qmnhC03nn",
        "outputId": "1c379824-bed7-456a-b2e2-ed4823fb1d56"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'are': 1, 'and': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_count3 = Counter(tkn3)\n",
        "stopword_count3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3VcykvJ1sGc",
        "outputId": "752d6d8d-b277-4127-af79-43c855fefbc5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'are': 1, 'and': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_count3 = Counter(tkn3)\n",
        "stopword_count3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05zNxdU-253U",
        "outputId": "d2a43fbd-f3a5-4fee-88b4-6daf691a4b51"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'are': 1, 'and': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tk1 = [token for token in token1 if token not in stopwords]\n",
        "tk2 = [token for token in token2 if token not in stopwords]\n",
        "tk3 = [token for token in token3 if token not in stopwords]\n",
        "\n",
        "print(tk1)\n",
        "print(tk2)\n",
        "print(tk3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZHSxqr127pO",
        "outputId": "bb8e4da2-5173-4be6-e5bd-b1db6fbe7873"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dogs', 'cute', 'loyal']\n",
            "['dogs', 'cute', 'cheerful']\n",
            "['dogs', 'playful', 'loyal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect= TfidfVectorizer()\n",
        "vect1=vect.fit_transform(tk1)\n",
        "vect2=vect.fit_transform(tk2)\n",
        "vect3=vect.fit_transform(tk3)\n",
        "print(vect1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2U_-UB_3WGx",
        "outputId": "7f5086c4-8256-404d-b856-d702b42c3076"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1)\t1.0\n",
            "  (1, 0)\t1.0\n",
            "  (2, 2)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqW5YG8r4n1s",
        "outputId": "95555194-391b-47d3-a282-8fe73fd8ba19"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vect2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFXeBq184VS_",
        "outputId": "24ffa17c-94bb-413e-a434-ed59ae8f7e07"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 2)\t1.0\n",
            "  (1, 1)\t1.0\n",
            "  (2, 0)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vect3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwdOAf-c4Zqm",
        "outputId": "c0eeed17-bd33-4a2f-8782-3659cce4505a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t1.0\n",
            "  (1, 2)\t1.0\n",
            "  (2, 1)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim=cosine_similarity(vect1, vect2)\n",
        "print(sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK1ZTHxh3ePh",
        "outputId": "e6ae1cdf-1be7-4fcd-e7f8-6e134c47356e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cos_dist=1-sim\n",
        "print(cos_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Aso71SB4GhX",
        "outputId": "e84c4394-f0dc-456c-d986-a80f4435a290"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 1.]\n",
            " [1. 1. 0.]\n",
            " [0. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#manual script of finding cosine similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk. corpus import stopwords\n",
        "text_doc1=\"dogs are cute and loyal\"\n",
        "text_doc2=\"dogs are cute and cheerful\"\n",
        "text_doc3=\"dogs are playful and loyal\"\n",
        "\n",
        "#tokenize the sentence\n",
        "token1=word_tokenize(text_doc1)\n",
        "print(\"tokenizedtext_doc1:\",token1)\n",
        "token2=word_tokenize(text_doc2)\n",
        "print(\"tokenizedtext_doc2:\",token2)\n",
        "token3=word_tokenize(text_doc3)\n",
        "print(\"tokenizedtext_doc3:\",token3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEObtVF99RoE",
        "outputId": "f6ae4c56-5528-4271-b776-08048d6d413a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizedtext_doc1: ['dogs', 'are', 'cute', 'and', 'loyal']\n",
            "tokenizedtext_doc2: ['dogs', 'are', 'cute', 'and', 'cheerful']\n",
            "tokenizedtext_doc3: ['dogs', 'are', 'playful', 'and', 'loyal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = list(stopwords.words('english'))\n",
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM-1swI9CjV4",
        "outputId": "c52725b7-ba89-4a14-b0c5-d30f1caf1b03"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1 =[];l2 =[]\n",
        "tkn1 = [w for w in token1 if not w in stopwords]\n",
        "tkn2 = [w for w in token2 if not w in stopwords]\n",
        "rvect=list(set(tkn1).union(set(tkn2)))\n",
        "for token in rvect:\n",
        "  if token in tkn1:l1.append(1)\n",
        "  else:l1.append(0)\n",
        "  if token in tkn2:l2.append(1)\n",
        "  else:l2.append(0)\n",
        "c=0\n",
        "\n",
        "for i in range(len(rvect)):\n",
        "        c+= l1[i]*l2[i]\n",
        "cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "print(\"similarity: \", cosine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-uBRq6DCtnN",
        "outputId": "ea962927-7d7c-4860-ba94-5aecce0803e3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity:  0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1 =[];l2 =[]\n",
        "tkn2 = [w for w in token2 if not w in stopwords]\n",
        "tkn3 = [w for w in token3 if not w in stopwords]\n",
        "rvect=list(set(tkn2).union(set(tkn3)))\n",
        "for token in rvect:\n",
        "  if token in tkn2:l1.append(1)\n",
        "  else:l1.append(0)\n",
        "  if token in tkn3:l2.append(1)\n",
        "  else:l2.append(0)\n",
        "c=0\n",
        "\n",
        "for i in range(len(rvect)):\n",
        "        c+= l1[i]*l2[i]\n",
        "cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "print(\"similarity: \", cosine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcCRkuHwCv8p",
        "outputId": "18d8084b-4463-437d-d951-de9bc5fed542"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity:  0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#jaccard similarity\n",
        "def jaccard_sim(a,b):\n",
        "  intersection = len(a.intersection(b))\n",
        "  union=len(a.union(b))\n",
        "  return intersection / union\n",
        "\n",
        "similarity = jaccard_sim(a, b)\n",
        "print(\"Jaccard Similarity:\", similarity)\n"
      ],
      "metadata": {
        "id": "V6cN0gBfFWTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3871e596-a8c9-4b52-a793-4ad1a74c494b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1=\"dogs are cute and loyal\"\n",
        "doc2=\"dogs are cute and cheerful\"\n",
        "doc3=\"dogs are playful and loyal\"\n",
        "print(doc1)\n",
        "print(doc2)\n",
        "print(doc3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK5dnRdY59Ks",
        "outputId": "fc9887d7-aff2-4bdd-b2c6-9b1b8ad8d591"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dogs are cute and loyal\n",
            "dogs are cute and cheerful\n",
            "dogs are playful and loyal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#minkowski , euclidean and hamming distance\n",
        "vectorize=CountVectorizer()"
      ],
      "metadata": {
        "id": "xx8AAxdDFoac"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "euc_vect=vectorize.fit_transform([doc1,doc2,doc3])\n",
        "euc_dist_mat=euclidean_distances(euc_vect)\n",
        "print(\"euclidean Distance matrix:\",euc_dist_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZfr7-np8V7l",
        "outputId": "844429bc-4eca-44e7-c47f-de9c27765090"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "euclidean Distance matrix: [[0.         1.41421356 1.41421356]\n",
            " [1.41421356 0.         2.        ]\n",
            " [1.41421356 2.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embedding is a language modeling technique for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks, co-occurrence matrices, probabilistic models, etc. Word2Vec consists of models for generating word embedding.\n",
        "Word2Vec is a widely used method in natural language processing (NLP) that allows words to be represented as vectors in a continuous vector space.\n"
      ],
      "metadata": {
        "id": "gGCOAURftFT8"
      }
    }
  ]
}